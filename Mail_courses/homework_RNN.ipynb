{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD \n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Упражнение, для реализации \"Ванильной\" RNN\n",
    "* Попробуем обучить сеть восстанавливать слово hello по первой букве. т.е. построим charecter-level модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((3,3))*3\n",
    "b = torch.ones((3,3))*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[45., 45., 45.],\n",
       "        [45., 45., 45.],\n",
       "        [45., 45., 45.]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 15., 15.],\n",
       "        [15., 15., 15.],\n",
       "        [15., 15., 15.]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word = 'ololoasdasddqweqw123456789'\n",
    "word = 'hello'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Датасет. \n",
    "Позволяет:\n",
    "* Закодировать символ при помощи one-hot\n",
    "* Делать итератор по слову, которыей возвращает текущий символ и следующий как таргет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataSet:\n",
    "    \n",
    "    def __init__(self, word):\n",
    "        self.chars2idx = {}\n",
    "        self.indexs  = []\n",
    "        for c in word: \n",
    "            if c not in self.chars2idx:\n",
    "                self.chars2idx[c] = len(self.chars2idx)\n",
    "                \n",
    "            self.indexs.append(self.chars2idx[c])\n",
    "            \n",
    "        self.vec_size = len(self.chars2idx)\n",
    "        self.seq_len  = len(word)\n",
    "        \n",
    "    def get_one_hot(self, idx):\n",
    "        x = torch.zeros(self.vec_size)\n",
    "        x[idx] = 1\n",
    "        return x\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return zip(self.indexs[:-1], self.indexs[1:])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.seq_len\n",
    "    \n",
    "    def get_char_by_id(self, id):\n",
    "        for c, i in self.chars2idx.items():\n",
    "            if id == i: return c\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация базовой RNN\n",
    "<br/>\n",
    "Скрытый элемент\n",
    "$$ h_t= tanh⁡ (W_{ℎℎ} h_{t−1}+W_{xh} x_t) $$\n",
    "Выход сети\n",
    "\n",
    "$$ y_t = W_{hy} h_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
    "        super(VanillaRNN, self).__init__()        \n",
    "        self.x2hidden    = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden      = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.activation  = nn.Tanh()\n",
    "        self.outweight   = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "    \n",
    "    def forward(self, x, prev_hidden):\n",
    "        hidden = self.activation(self.x2hidden(x) + self.hidden(prev_hidden))\n",
    "#         Версия без активации - может происходить gradient exploding\n",
    "#         hidden = self.x2hidden(x) + self.hidden(prev_hidden)\n",
    "        output = self.outweight(hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация переменных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "rnn = VanillaRNN(in_size=ds.vec_size, hidden_size=3, out_size=ds.vec_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "e_cnt     = 100\n",
    "optim     = SGD(rnn.parameters(), lr = 0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.082126140594482\n",
      "Clip gradient :  2.6229139269528163\n",
      "5.00617790222168\n",
      "Clip gradient :  1.6478035554009303\n",
      "3.9959566593170166\n",
      "Clip gradient :  1.1804787072021505\n",
      "3.096665859222412\n",
      "Clip gradient :  1.0647933660579545\n",
      "2.2841830253601074\n",
      "Clip gradient :  0.8430615592995833\n",
      "1.7259869575500488\n",
      "Clip gradient :  0.7169644919648777\n",
      "1.2411096096038818\n",
      "Clip gradient :  0.9171844458764893\n",
      "0.7921334505081177\n",
      "Clip gradient :  0.5959908016834634\n",
      "0.49054455757141113\n",
      "Clip gradient :  0.48165733988044906\n",
      "0.30702996253967285\n",
      "Clip gradient :  0.32871085914106685\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = True\n",
    "\n",
    "for epoch in range(e_cnt):\n",
    "    hh = torch.zeros(rnn.hidden.in_features)\n",
    "    loss = 0\n",
    "    optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "\n",
    "        y, hh = rnn(x, hh)\n",
    "        loss += criterion(y, target)\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1)\n",
    "            \n",
    "#     print(\"Params : \")\n",
    "#     num_params = 0\n",
    "#     for item in rnn.parameters():\n",
    "#         num_params += 1\n",
    "#         print(item.grad)\n",
    "#     print(\"NumParams :\", num_params)\n",
    "#     print(\"Optimize\")\n",
    "    \n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t hello\n",
      "Original:\t hello\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "rnn.eval()\n",
    "hh = torch.zeros(rnn.hidden.in_features)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh = rnn(x, hh)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "try:\n",
    "    assert(predword == word)\n",
    "    print('Done')\n",
    "except AssertionError:\n",
    "    print('Ошибка, слова не равны.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ\n",
    "Реализовать LSTM и GRU модули, обучить их предсказывать тестовое слово"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#тестовое слово\n",
    "word = 'ololoasdasddqweqw123456789'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_RNN2(nn.Module):\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        super(LSTM_RNN2, self).__init__()\n",
    "        self.forget_h = nn.Linear(in_features=size, out_features=size)\n",
    "        self.forget_x = nn.Linear(in_features=size, out_features=size)\n",
    "        self.input_h = nn.Linear(in_features=size, out_features=size)\n",
    "        self.input_x = nn.Linear(in_features=size, out_features=size)\n",
    "        self.candidate_h = nn.Linear(in_features=size, out_features=size)\n",
    "        self.candidate_x = nn.Linear(in_features=size, out_features=size)\n",
    "        self.output_h = nn.Linear(in_features=size, out_features=size)\n",
    "        self.output_x = nn.Linear(in_features=size, out_features=size)\n",
    "        self.out = nn.Linear(in_features=size, out_features=size)\n",
    "        \n",
    "        self.sigma  = nn.Sigmoid()\n",
    "        self.tanh  = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x, prev_hidden, prev_state):\n",
    "        forget_gate = self.sigma(self.forget_x(x) + self.forget_h(prev_hidden)) #возм по-другому сканкатенировать!\n",
    "        input_gate = self.sigma(self.input_x(x) + self.input_h(prev_hidden))\n",
    "        candidate_gate = self.tanh(self.candidate_x(x) + self.candidate_h(prev_hidden))\n",
    "        output_gate = self.sigma(self.output_x(x) + self.output_h(prev_hidden))\n",
    "        \n",
    "        state = input_gate*candidate_gate + prev_state*forget_gate\n",
    "        hidden = output_gate * self.tanh(state)\n",
    "        out = self.out(hidden)\n",
    "        \n",
    "        return out, hidden, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm2\n",
    "ds = WordDataSet(word=word)\n",
    "lstm2 = LSTM_RNN2(size=ds.vec_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optim     = SGD(lstm2.parameters(), lr = 0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586d1ed186024298915abd5230d50a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clip gradient :  3.154193471138999\n",
      "Clip gradient :  2.3480208174299255\n",
      "Clip gradient :  1.6425997359057256\n",
      "Clip gradient :  2.168367882250224\n",
      "Clip gradient :  3.4515866052645316\n",
      "Clip gradient :  5.132345389238131\n",
      "Clip gradient :  5.8200783432933445\n",
      "Clip gradient :  5.401052799168868\n",
      "Clip gradient :  4.812899602703747\n",
      "Clip gradient :  4.06251725895309\n",
      "Clip gradient :  4.016242873838575\n",
      "Clip gradient :  3.5334145065760683\n",
      "Clip gradient :  3.1424755307576775\n",
      "Clip gradient :  3.1858664791603726\n",
      "Clip gradient :  3.4250711062348826\n",
      "Clip gradient :  5.12973538838773\n",
      "Clip gradient :  17.268832791224202\n",
      "Clip gradient :  9.756044232129671\n",
      "Clip gradient :  6.576823306437076\n",
      "Clip gradient :  12.911056760911341\n",
      "Clip gradient :  11.788825665238434\n",
      "Clip gradient :  6.467368173142871\n",
      "Clip gradient :  14.53787341511306\n",
      "Clip gradient :  17.242254655802697\n",
      "Clip gradient :  10.201136447735099\n",
      "Clip gradient :  11.947438809630015\n",
      "Clip gradient :  9.052965581096203\n",
      "Clip gradient :  9.920396349214661\n",
      "Clip gradient :  7.493497509460253\n",
      "Clip gradient :  6.745127112730922\n",
      "Clip gradient :  5.349429526770371\n",
      "Clip gradient :  4.773996620185013\n",
      "Clip gradient :  3.5610140321034875\n",
      "Clip gradient :  2.2278587711800655\n",
      "Clip gradient :  1.0558032568764724\n",
      "Clip gradient :  0.665654776219928\n",
      "Clip gradient :  0.5155255126234791\n",
      "Clip gradient :  0.4097354489171992\n",
      "Clip gradient :  0.34351923770970816\n",
      "Clip gradient :  0.29466270005028655\n",
      "Clip gradient :  0.2579578814042691\n",
      "Clip gradient :  0.2297826191236983\n",
      "Clip gradient :  0.2076117439872372\n",
      "Clip gradient :  0.18936470905725633\n",
      "Clip gradient :  0.174069432303599\n",
      "Clip gradient :  0.1610879777198892\n",
      "Clip gradient :  0.14990481461101013\n",
      "Clip gradient :  0.1401661677992339\n",
      "Clip gradient :  0.1316107939857819\n",
      "Clip gradient :  0.12403212866325093\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = True\n",
    "\n",
    "for epoch in tqdm_notebook(range(500)):\n",
    "    hh = torch.zeros(lstm2.forget_h.in_features)\n",
    "    state = torch.zeros(lstm2.candidate_h.in_features)\n",
    "    loss = 0\n",
    "    optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "\n",
    "        y, hh, state = lstm2(x, hh, state)\n",
    "        loss += criterion(y, target)\n",
    "\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        #print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(lstm2.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(lstm2.parameters(), max_norm=1)\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t ololoasdasddqweqw123456789\n",
      "Original:\t ololoasdasddqweqw123456789\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "lstm2.eval()\n",
    "\n",
    "hh = torch.zeros(lstm2.forget_h.in_features)\n",
    "state = torch.zeros(lstm2.candidate_h.in_features)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    #y, hh = rnn(x, hh)\n",
    "    y, hh, state = lstm2(x, hh, state)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "try:\n",
    "    assert(predword == word)\n",
    "    print('Done!')\n",
    "except AssertionError:\n",
    "    print('Ошибка, слова не равны.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Написать реализацию GRU и обучить предсказывать слово"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        super(GRU_RNN, self).__init__()\n",
    "        self.reset_h = nn.Linear(in_features=size, out_features=size)\n",
    "        self.reset_x = nn.Linear(in_features=size, out_features=size)\n",
    "        self.candidate_h = nn.Linear(in_features=size, out_features=size)\n",
    "        self.candidate_x = nn.Linear(in_features=size, out_features=size)\n",
    "        self.update_h = nn.Linear(in_features=size, out_features=size)\n",
    "        self.update_x = nn.Linear(in_features=size, out_features=size)\n",
    "        self.out = nn.Linear(in_features=size, out_features=size)\n",
    "        \n",
    "        self.sigma  = nn.Sigmoid()\n",
    "        self.tanh  = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x, prev_hidden):\n",
    "        reset_gate = self.sigma(self.reset_x(x) + self.reset_h(prev_hidden)) \n",
    "        candidate_gate = self.tanh(self.candidate_x(x) + self.candidate_h(prev_hidden)*reset_gate)\n",
    "        update_gate = self.sigma(self.update_x(x) + self.update_h(prev_hidden))\n",
    "        \n",
    "        hidden = prev_hidden*update_gate + candidate_gate*(1 - update_gate)\n",
    "        out = self.out(hidden)\n",
    "        \n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "gru = GRU_RNN(size=ds.vec_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optim     = SGD(gru.parameters(), lr = 0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc0f9e147b34c3b82b4e3fbac1e8f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.3506088256836\n",
      "Clip gradient :  3.9861658095113714\n",
      "68.17176055908203\n",
      "Clip gradient :  3.331887899431424\n",
      "65.17427825927734\n",
      "Clip gradient :  3.4763075090123103\n",
      "60.93827819824219\n",
      "Clip gradient :  4.802023790606661\n",
      "54.364723205566406\n",
      "Clip gradient :  6.459210915494356\n",
      "46.05052947998047\n",
      "Clip gradient :  6.516791412976152\n",
      "38.55032730102539\n",
      "Clip gradient :  5.477175539889719\n",
      "32.22412109375\n",
      "Clip gradient :  4.916311989517177\n",
      "26.48287582397461\n",
      "Clip gradient :  4.40462455604256\n",
      "21.3674259185791\n",
      "Clip gradient :  3.8924844990414993\n",
      "16.86004638671875\n",
      "Clip gradient :  3.4794756942282747\n",
      "12.921923637390137\n",
      "Clip gradient :  3.0371855218892865\n",
      "9.625797271728516\n",
      "Clip gradient :  2.5691545907699562\n",
      "6.971035480499268\n",
      "Clip gradient :  2.1099869738775707\n",
      "4.899784088134766\n",
      "Clip gradient :  1.6839306449513036\n",
      "3.313459873199463\n",
      "Clip gradient :  1.3564478050977262\n",
      "2.08853816986084\n",
      "Clip gradient :  1.0665830676073547\n",
      "1.2528347969055176\n",
      "Clip gradient :  0.7058360563358445\n",
      "0.8165812492370605\n",
      "Clip gradient :  0.47461903843855635\n",
      "0.5906949043273926\n",
      "Clip gradient :  0.348393761904137\n",
      "0.46209287643432617\n",
      "Clip gradient :  0.27570369343284035\n",
      "0.38117074966430664\n",
      "Clip gradient :  0.22979066299978917\n",
      "0.3256797790527344\n",
      "Clip gradient :  0.19817351160916843\n",
      "0.2850017547607422\n",
      "Clip gradient :  0.1748406083956422\n",
      "0.25368785858154297\n",
      "Clip gradient :  0.15673397099493871\n",
      "\n",
      "finished training\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = True\n",
    "e_cnt = 250\n",
    "\n",
    "for epoch in tqdm_notebook(range(e_cnt)):\n",
    "    hh = torch.zeros(gru.reset_h.in_features)\n",
    "    loss = 0\n",
    "    optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "\n",
    "        y, hh = gru(x, hh)\n",
    "        loss += criterion(y, target)\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(gru.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(gru.parameters(), max_norm=1)\n",
    "    \n",
    "    optim.step()\n",
    "\n",
    "print('finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t ololoasdasddqweqw123456789\n",
      "Original:\t ololoasdasddqweqw123456789\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#оценка gru\n",
    "\n",
    "gru.eval()\n",
    "hh = torch.zeros(gru.reset_h.in_features)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh = gru(x, hh)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "try:\n",
    "    assert(predword == word)\n",
    "    print('Done!')\n",
    "except AssertionError:\n",
    "    print('Ошибка, слова не равны.')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
